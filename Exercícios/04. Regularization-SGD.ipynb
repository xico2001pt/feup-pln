{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt).\n",
    "\n",
    "# REGULARIZATION AND SGD\n",
    "\n",
    "Regularization is a technique that allows us to avoid overfitting by penalizing excessive feature weights. Several classifiers, such as [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html),  include the option for choosing which regularization term to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll explore the usage of different regularization terms. For that, we'll use a restaurant reviews classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    500\n",
      "0    500\n",
      "Name: Liked, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('data/restaurant_reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "print(dataset['Liked'].value_counts())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "for i in range(0,1000):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    # to lower-case and tokenize\n",
    "    review = review.lower().split()\n",
    "    # stemming and stop word removal\n",
    "    review = ' '.join([ps.stem(w) for w in review if not w in set(stopwords.words('english'))])\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1500) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Creating a bag-of-words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1500)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset['Liked']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1500) (800,)\n",
      "(200, 1500) (200,)\n",
      "1    400\n",
      "0    400\n",
      "Name: Liked, dtype: int64\n",
      "0    100\n",
      "1    100\n",
      "Name: Liked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Scikit-learn's [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) includes both L1 and L2 regularizations. L2 is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83 17]\n",
      " [22 78]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = LogisticRegression(penalty='l2') # l2 regularization is the default\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the feature weights that we've obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41686797  0.17697687  0.         ... -0.20321363  0.64383437\n",
      "  -0.61415454]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features are actually being used? (I.e., how many non-zero weights are there?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features:  1500\n",
      "Used features:  1311\n"
     ]
    }
   ],
   "source": [
    "total_features = len(clf.coef_[0])\n",
    "\n",
    "used_features = sum([1 for i in clf.coef_[0] if i != 0])\n",
    "\n",
    "print('Total features: ', total_features)\n",
    "print('Used features: ', used_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization typically obtains sparser weight vectors. Try using L1 regularization (check the documentation for additional changes you might need). How many non-zero weights do you have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89 11]\n",
      " [30 70]]\n",
      "Total features:  1500\n",
      "Used features:  149\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Total features: \", len(clf.coef_[0]))\n",
    "print(\"Used features: \", sum([1 for i in clf.coef_[0] if i != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try using a mix of L1 and L2 (check the documentation for how to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84 16]\n",
      " [28 72]]\n",
      "Total features:  1500\n",
      "Used features:  380\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Total features: \", len(clf.coef_[0]))\n",
    "print(\"Used features: \", sum([1 for i in clf.coef_[0] if i != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Scikit-learn's [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) also includes both L1 and L2 regularizations. L2 is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82 18]\n",
      " [20 80]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = LinearSVC(penalty='l2') # l2 regularization is the default\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features are actually being used? (I.e., how many non-zero weights are there?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features:  1500\n",
      "Used features:  1084\n"
     ]
    }
   ],
   "source": [
    "print(\"Total features: \", len(clf.coef_[0]))\n",
    "print(\"Used features: \", sum([1 for i in clf.coef_[0] if i != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using L1 regularization (check the documentation for additional changes you might need). How many non-zero weights do you have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86 14]\n",
      " [27 73]]\n",
      "Total features:  1500\n",
      "Used features:  416\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(penalty='l1', dual=False)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Total features: \", len(clf.coef_[0]))\n",
    "print(\"Used features: \", sum([1 for i in clf.coef_[0] if i != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "\n",
    "Scikit-learn's [SGD Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) implements regularized linear models (such as SVM and Logistic Regression) with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing learning rate.\n",
    "\n",
    "Several loss functions can be used, namely *hinge loss* (which corresponds to SVM) and *log loss* (which corresponds to Logistic Regression). And as before, you can use L1 and/or L2 regularization.\n",
    "\n",
    "The *max_iter* parameter allows you to set the maximum number of epochs, where an epoch corresponds to going through the whole dataset for training. Also, *learning_rate* allows you to set a learning rate schedule.\n",
    "\n",
    "Several parameters allow you to define stopping criteria: *tol* specifies a tolerance loss value or stopping criterion, while *n_iter_no_change* indicates the number of iterations with no improvement that should be observed before stopping; *early_stopping* allows us to use a validation set (a fraction *validation_fraction* of the training data) on which the stopping criterion will be checked (instead of checking the loss on the training data).\n",
    "\n",
    "The *verbose* parameter allows you to set a verbosity (output) level.\n",
    "\n",
    "Try using SGD, and explore different parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78 22]\n",
      " [19 81]]\n",
      "Total features:  1500\n",
      "Used features:  1096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Total features: \", len(clf.coef_[0]))\n",
    "print(\"Used features: \", sum([1 for i in clf.coef_[0] if i != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent updates the model weights base on one example at a time. Instead, we can compute the gradient over batches of training instances before updating the weights.\n",
    "\n",
    "SGDClassifier allows us to do so via [*partial_fit*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.partial_fit), which corresponds to training the model with a specific set of examples for a single epoch. To properly use this method, we need to split our data into mini-batches and then iterate through them for as many epochs as we want.\n",
    "Matters such as objective convergence, early stopping, and learning rate adjustments must be handled manually.\n",
    "\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test Accuracy =  0.750000\n",
      "Epoch 1: Test Accuracy =  0.770000\n",
      "Epoch 2: Test Accuracy =  0.810000\n",
      "Epoch 3: Test Accuracy =  0.800000\n",
      "Epoch 4: Test Accuracy =  0.810000\n",
      "Epoch 5: Test Accuracy =  0.820000\n",
      "Epoch 6: Test Accuracy =  0.815000\n",
      "Epoch 7: Test Accuracy =  0.815000\n",
      "Epoch 8: Test Accuracy =  0.815000\n",
      "Epoch 9: Test Accuracy =  0.820000\n",
      "Epoch 10: Test Accuracy =  0.825000\n",
      "Epoch 11: Test Accuracy =  0.825000\n",
      "Epoch 12: Test Accuracy =  0.820000\n",
      "Epoch 13: Test Accuracy =  0.815000\n",
      "Epoch 14: Test Accuracy =  0.815000\n",
      "Epoch 15: Test Accuracy =  0.810000\n",
      "Epoch 16: Test Accuracy =  0.815000\n",
      "Epoch 17: Test Accuracy =  0.815000\n",
      "Epoch 18: Test Accuracy =  0.815000\n",
      "Epoch 19: Test Accuracy =  0.815000\n",
      "Epoch 20: Test Accuracy =  0.815000\n",
      "Epoch 21: Test Accuracy =  0.815000\n",
      "Epoch 22: Test Accuracy =  0.815000\n",
      "Epoch 23: Test Accuracy =  0.820000\n",
      "Epoch 24: Test Accuracy =  0.820000\n",
      "Epoch 25: Test Accuracy =  0.825000\n",
      "Epoch 26: Test Accuracy =  0.820000\n",
      "Epoch 27: Test Accuracy =  0.820000\n",
      "Epoch 28: Test Accuracy =  0.820000\n",
      "Epoch 29: Test Accuracy =  0.815000\n",
      "Epoch 30: Test Accuracy =  0.815000\n",
      "Epoch 31: Test Accuracy =  0.815000\n",
      "Epoch 32: Test Accuracy =  0.815000\n",
      "Epoch 33: Test Accuracy =  0.815000\n",
      "Epoch 34: Test Accuracy =  0.815000\n",
      "Epoch 35: Test Accuracy =  0.815000\n",
      "Epoch 36: Test Accuracy =  0.815000\n",
      "Epoch 37: Test Accuracy =  0.815000\n",
      "Epoch 38: Test Accuracy =  0.815000\n",
      "Epoch 39: Test Accuracy =  0.820000\n",
      "Epoch 40: Test Accuracy =  0.820000\n",
      "Epoch 41: Test Accuracy =  0.820000\n",
      "Epoch 42: Test Accuracy =  0.820000\n",
      "Epoch 43: Test Accuracy =  0.820000\n",
      "Epoch 44: Test Accuracy =  0.820000\n",
      "Epoch 45: Test Accuracy =  0.820000\n",
      "Epoch 46: Test Accuracy =  0.820000\n",
      "Epoch 47: Test Accuracy =  0.820000\n",
      "Epoch 48: Test Accuracy =  0.820000\n",
      "Epoch 49: Test Accuracy =  0.820000\n",
      "Epoch 50: Test Accuracy =  0.820000\n",
      "Epoch 51: Test Accuracy =  0.815000\n",
      "Epoch 52: Test Accuracy =  0.815000\n",
      "Epoch 53: Test Accuracy =  0.815000\n",
      "Epoch 54: Test Accuracy =  0.815000\n",
      "Epoch 55: Test Accuracy =  0.815000\n",
      "Epoch 56: Test Accuracy =  0.815000\n",
      "Epoch 57: Test Accuracy =  0.815000\n",
      "Epoch 58: Test Accuracy =  0.815000\n",
      "Epoch 59: Test Accuracy =  0.815000\n",
      "Epoch 60: Test Accuracy =  0.815000\n",
      "Epoch 61: Test Accuracy =  0.815000\n",
      "Epoch 62: Test Accuracy =  0.815000\n",
      "Epoch 63: Test Accuracy =  0.815000\n",
      "Epoch 64: Test Accuracy =  0.815000\n",
      "Epoch 65: Test Accuracy =  0.815000\n",
      "Epoch 66: Test Accuracy =  0.815000\n",
      "Epoch 67: Test Accuracy =  0.815000\n",
      "Epoch 68: Test Accuracy =  0.815000\n",
      "Epoch 69: Test Accuracy =  0.815000\n",
      "Epoch 70: Test Accuracy =  0.815000\n",
      "Epoch 71: Test Accuracy =  0.815000\n",
      "Epoch 72: Test Accuracy =  0.815000\n",
      "Epoch 73: Test Accuracy =  0.815000\n",
      "Epoch 74: Test Accuracy =  0.815000\n",
      "Epoch 75: Test Accuracy =  0.815000\n",
      "Epoch 76: Test Accuracy =  0.815000\n",
      "Epoch 77: Test Accuracy =  0.820000\n",
      "Epoch 78: Test Accuracy =  0.820000\n",
      "Epoch 79: Test Accuracy =  0.820000\n",
      "Epoch 80: Test Accuracy =  0.820000\n",
      "Epoch 81: Test Accuracy =  0.825000\n",
      "Epoch 82: Test Accuracy =  0.825000\n",
      "Epoch 83: Test Accuracy =  0.825000\n",
      "Epoch 84: Test Accuracy =  0.825000\n",
      "Epoch 85: Test Accuracy =  0.825000\n",
      "Epoch 86: Test Accuracy =  0.825000\n",
      "Epoch 87: Test Accuracy =  0.825000\n",
      "Epoch 88: Test Accuracy =  0.825000\n",
      "Epoch 89: Test Accuracy =  0.820000\n",
      "Epoch 90: Test Accuracy =  0.820000\n",
      "Epoch 91: Test Accuracy =  0.820000\n",
      "Epoch 92: Test Accuracy =  0.820000\n",
      "Epoch 93: Test Accuracy =  0.820000\n",
      "Epoch 94: Test Accuracy =  0.820000\n",
      "Epoch 95: Test Accuracy =  0.820000\n",
      "Epoch 96: Test Accuracy =  0.820000\n",
      "Epoch 97: Test Accuracy =  0.820000\n",
      "Epoch 98: Test Accuracy =  0.820000\n",
      "Epoch 99: Test Accuracy =  0.820000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "batch_size = 32\n",
    "n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        clf.partial_fit(X_batch, y_batch, classes=np.unique(y))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Epoch {epoch}: Test Accuracy = {accuracy_score(y_test, y_pred): 2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
