{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt).\n",
    "\n",
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "*Tokenization* is the process of spliting an input text into tokens (words or other relevant elements, such as punctuation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making use of regular expressions\n",
    "\n",
    "We can tokenize a piece of text by using a regular expression tokenizer, such as the one available in **NLTK**.\n",
    "\n",
    "For starters, let's stick to alphanumerical sequences of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "pattern = '[a-zA-Z0-9_]+'\n",
    "tokens = regexp_tokenize(text, pattern)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refine the regular expression to obtain a more sensible tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)           # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
    "        | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "        | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "        | \\.\\.\\.             # ellipsis\n",
    "        | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "        '''\n",
    "\n",
    "tokens = regexp_tokenize(text, pattern)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK\n",
    "\n",
    "NLTK also includes a word tokenizer, which gets roughly the same result (it finds \"words\" and punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'think', 'we', \"'re\", 'flying', 'today', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I don't think we're flying today.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try [other tokenizers](https://www.nltk.org/api/nltk.tokenize.html) available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'don', \"'\", 't', 'think', 'we', \"'\", 're', 'flying', 'today', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out the wordpunct tokenizer\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(\"I don't think we're flying today.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sentence from the user and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You typed 0 words: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "s = input(\"Enter some text:\")\n",
    "tokens = word_tokenize(s)\n",
    "\n",
    "print(\"You typed\", len(tokens), \"words:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence segmentation\n",
    "\n",
    "We may also be interested in spliting the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'Are you Mr. Smith?', 'Just to let you know that I have finished my M.Sc.', 'and Ph.D. on AI.', 'I loved it!']\n",
      "Number of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text = \"Hello. Are you Mr. Smith? Just to let you know that I have finished my M.Sc. and Ph.D. on AI. I loved it!\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n",
    "print(\"Number of sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with long texts\n",
    "\n",
    "We can try downloading a book from [Project Gutenberg](https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176812\n",
      "ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "print(len(raw))\n",
    "print(raw[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sentences are there? Printout the second sentence (index 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 12060\n",
      "Second sentence: You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(raw)\n",
    "\n",
    "print(f'Number of sentences: {len(sentences)}')\n",
    "print(f'Second sentence: {sentences[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tokens are there? What is the index of the first token in the second sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257058\n",
      "['You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "tokens_second_sentence = word_tokenize(sentences[1])\n",
    "\n",
    "print(len(tokens))\n",
    "print(tokens_second_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how many types (unique tokens) are there? Which is the most frequent one? *(Hint: use a [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) container from collections.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 16177)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(tokens)\n",
    "\n",
    "print(len(counter))\n",
    "counter.most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with multi-word expressions (MWE)\n",
    "\n",
    "Sometimes we want certain words to stick together when tokenizing, such as in multi-word names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Good muffins cost $3.88\\nin New York.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do it is to suply our own lexicon and make use of NLTK's [MWE tokenizer](https://www.nltk.org/api/nltk.tokenize.mwe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.\"\n",
    "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator=' ')\n",
    "\n",
    "[mwe.tokenize(word_tokenize(sent)) for sent in sent_tokenize(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "*Stemming* and *Lemmatization* are techniques used to normalize tokens, so as to reduce the size of the vocabulary.\n",
    "Whereas lemmatization is a process of finding the root of the word, stemming typically applies a set of transformation rules that aim to cut off word final affixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "NLTK includes one of the most well-known stemmers: the [Porter stemmer](https://www.emerald.com/insight/content/doi/10.1108/00330330610681286/full/pdf?casa_token=eT_IPtH_eLEAAAAA:Z3lAtxWdxf0FL479mL-A7tC-_QRzxNeeyC2DFLyWwGBlcj6DQcwu2Bnq37waDPcXKOnXkMMDtKGyCaYGZtYcb3lgBZ9uaHKUNO0JCMivSdPE4HTe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# initialize the Porter Stemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an illustrative piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original word list: ['The', 'European', 'Commission', 'has', 'funded', 'a', 'numerical', 'study', 'to', 'analyze', 'the', 'purchase', 'of', 'a', 'pipe', 'organ', 'with', 'no', 'noise', 'for', 'Europe', \"'s\", 'organization', '.', 'Numerous', 'donations', 'have', 'followed', 'the', 'analysis', 'after', 'a', 'noisy', 'debate', '.']\n",
      "\n",
      "Original number of distinct tokens: 31\n"
     ]
    }
   ],
   "source": [
    "sentence = '''The European Commission has funded a numerical study to analyze the purchase of a pipe organ with no noise\n",
    "for Europe's organization. Numerous donations have followed the analysis after a noisy debate.'''\n",
    "\n",
    "# tokenize: split the text into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(\"\\nOriginal word list:\", word_list)\n",
    "print(\"\\nOriginal number of distinct tokens:\", len(set(word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stem the tokens in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text: the european commiss ha fund a numer studi to analyz the purchas of a pipe organ with no nois for europ 's organ . numer donat have follow the analysi after a noisi debat .\n",
      "\n",
      "Stemmed word list: ['the', 'european', 'commiss', 'ha', 'fund', 'a', 'numer', 'studi', 'to', 'analyz', 'the', 'purchas', 'of', 'a', 'pipe', 'organ', 'with', 'no', 'nois', 'for', 'europ', \"'s\", 'organ', '.', 'numer', 'donat', 'have', 'follow', 'the', 'analysi', 'after', 'a', 'noisi', 'debat', '.']\n",
      "\n",
      "Stemmed number of distinct tokens: 28\n"
     ]
    }
   ],
   "source": [
    "# stem list of words and join\n",
    "stemmed_output = ' '.join([porter.stem(w) for w in word_list])\n",
    "print(\"Stemmed text:\", stemmed_output)\n",
    "\n",
    "# tokenize: split the text into words\n",
    "stemmed_word_list = nltk.word_tokenize(stemmed_output)\n",
    "\n",
    "print(\"\\nStemmed word list:\", stemmed_word_list)\n",
    "print(\"\\nStemmed number of distinct tokens:\", len(set(stemmed_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the reduced vocabulary size. Some tokens are over-generalized (semantically different tokens that get the same stem), while others are under-generalized (semantically similar tokens that get different stems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out [other stemmers](https://www.nltk.org/api/nltk.stem.html) available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed word list: ['the', 'european', 'commiss', 'has', 'fund', 'a', 'numer', 'studi', 'to', 'analyz', 'the', 'purchas', 'of', 'a', 'pipe', 'organ', 'with', 'no', 'nois', 'for', 'europ', \"'s\", 'organ', '.', 'numer', 'donat', 'have', 'follow', 'the', 'analysi', 'after', 'a', 'noisi', 'debat', '.']\n",
      "\n",
      "Stemmed number of distinct tokens: 28\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "new_stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "\n",
    "new_stemmed_word_list = nltk.word_tokenize(new_stemmed_output)\n",
    "\n",
    "print(\"\\nStemmed word list:\", new_stemmed_word_list)\n",
    "print(\"\\nStemmed number of distinct tokens:\", len(set(new_stemmed_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try a few for Portuguese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est mesm a gost dest unidad curricul , tod gost de unidad curricul interess .\n"
     ]
    }
   ],
   "source": [
    "# Portuguese stemmer: https://www.nltk.org/_modules/nltk/stem/rslp.html\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "sentence = \"Estou mesmo a gostar desta unidade curricular, todos gostamos de unidades curriculares interessantes.\"\n",
    "\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estou mesm a gost dest unidad curricul , tod gost de unidad curricul interess .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "sentence = \"Estou mesmo a gostar desta unidade curricular, todos gostamos de unidades curriculares interessantes.\"\n",
    "\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "NLTK includes a [lemmatizer based on WordNet](https://www.nltk.org/api/nltk.stem.wordnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men', 'and', 'women', 'love', 'to', 'study', 'artificial', 'intelligence', 'while', 'studying', 'data', 'science', '.', 'My', 'feet', 'and', 'teeth', 'are', 'clean', '!']\n",
      "['Men', 'and', 'woman', 'love', 'to', 'study', 'artificial', 'intelligence', 'while', 'studying', 'data', 'science', '.', 'My', 'foot', 'and', 'teeth', 'are', 'clean', '!']\n"
     ]
    }
   ],
   "source": [
    "# WordNet lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"Men and women love to study artificial intelligence while studying data science. My feet and teeth are clean!\"\n",
    "\n",
    "\n",
    "# tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "\n",
    "# lemmatize list of words\n",
    "lemmatized_output = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result with stemming applied to the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['men', 'and', 'women', 'love', 'to', 'studi', 'artifici', 'intellig', 'while', 'studi', 'data', 'scienc', '.', 'my', 'feet', 'and', 'teeth', 'are', 'clean', '!']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_output = [stemmer.stem(w) for w in word_list]\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "SpaCy includes several [language processing pipelines](https://spacy.io/usage/processing-pipelines) that streamline several NLP tasks at once. We can use one of the available [trained pipelines](https://spacy.io/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply pass the sentence through the language processing pipeline (in this case, for English):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men and women love to study artificial intelligence while studying data science. My feet and teeth are clean!\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "sent = nlp(sentence)\n",
    "print(sent)\n",
    "print(len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have a sequence of tokens, each of which has specific [attributes](https://spacy.io/api/token#attributes) attached. For instance, we can easily get the lemma for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men man\n",
      "and and\n",
      "women woman\n",
      "love love\n",
      "to to\n",
      "study study\n",
      "artificial artificial\n",
      "intelligence intelligence\n",
      "while while\n",
      "studying study\n",
      "data datum\n",
      "science science\n",
      ". .\n",
      "My my\n",
      "feet foot\n",
      "and and\n",
      "teeth tooth\n",
      "are be\n",
      "clean clean\n",
      "! !\n"
     ]
    }
   ],
   "source": [
    "for token in sent:\n",
    "    print(token.text, token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
